{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a0ca61",
   "metadata": {},
   "source": [
    "initialization and implementation of \"get_log_prob_of_a_given_path()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efd8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_of_a_given_path(path, sequence):\n",
    "    # Transition probabilities\n",
    "    trans = {\n",
    "        'E': {'E': math.log(0.9), '5': math.log(0.1), 'I': -math.inf, 'End': -math.inf},\n",
    "        '5': {'E': -math.inf, '5': -math.inf, 'I': math.log(1.0), 'End': -math.inf},\n",
    "        'I': {'E': -math.inf, '5': -math.inf, 'I': math.log(0.9), 'End': math.log(0.1)}\n",
    "    }\n",
    "    \n",
    "    # Emission probabilities\n",
    "    emit = {\n",
    "        'E': {'A': math.log(0.25), 'C': math.log(0.25), 'G': math.log(0.25), 'T': math.log(0.25)},\n",
    "        '5': {'A': math.log(0.05), 'C': -math.inf, 'G': math.log(0.95), 'T': -math.inf},\n",
    "        'I': {'A': math.log(0.4), 'C': math.log(0.1), 'G': math.log(0.1), 'T': math.log(0.4)}\n",
    "    }\n",
    "    \n",
    "    ans = 0.0\n",
    "    \n",
    "    # Initial state (Start -> First state in path)\n",
    "    first_state=path[0]\n",
    "    ans += math.log(1.0)  # Start -> E (forced by model)\n",
    "    ans += emit[first_state][sequence[0]]  # First emission\n",
    "    \n",
    "    # Iterate through the rest of the path\n",
    "    for i in range(1, len(path)):\n",
    "        prev_state=path[i-1]\n",
    "        curr_state=path[i]\n",
    "        ans+=trans[prev_state][curr_state]  # Transition\n",
    "        ans+=emit[curr_state][sequence[i]]  # Emission\n",
    "    \n",
    "    # Transition to End (if last state is 'I')\n",
    "    if path[-1]== 'I':\n",
    "        ans +=math.log(0.1)  # I -> End\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13dcfee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-41.21967768602254\n"
     ]
    }
   ],
   "source": [
    "# Example from the primer\n",
    "path = \"EEEEEEEEEEEEEEEEEE5IIIIIII\"\n",
    "sequence =\"CTTCATGTGAAAGCAGACGTAAGTCA\"\n",
    "print(get_log_prob_of_a_given_path(path, sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd25a2c",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "Say we're playing a guessing game with a friend who has a toy hidden in their hands. You can't see the toy, but your friend makes sounds that give you clues, like \"meow\" or \"woof.\" You want to guess which toy it is â€“ a cat or a dog.\n",
    "\n",
    "The Viterbi algorithm is like a super smart guesser. It listens to all the sounds your friend makes and tries to figure out the best guess for the hidden toy at each sound. It remembers all the sounds and figures out the most likely way your friend was thinking (\"Maybe first they thought of the cat, then the dog!\"). \n",
    "\n",
    "Let P be all possible tags at each stage and L be the length of input sequence \n",
    "\n",
    "Complexity of brute force: O(p^L) --> exponential \n",
    "\n",
    "Complexity of Viterbi: O(L*p^2) -->quadratic --> huge improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd0296",
   "metadata": {},
   "source": [
    "Implementation using Iterative Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sequence):\n",
    "    states = ['E', '5', 'I']\n",
    "    emissions = ['A', 'C', 'G', 'T']\n",
    "    \n",
    "    # Transition probabilities (log space)\n",
    "    trans = {\n",
    "        'Start': {'E': math.log(1.0), '5': -math.inf, 'I': -math.inf, 'End': -math.inf},\n",
    "        'E':{'E': math.log(0.9), '5': math.log(0.1), 'I': -math.inf, 'End': -math.inf},\n",
    "        '5':{'E': -math.inf, '5': -math.inf, 'I':math.log(1.0), 'End': -math.inf},\n",
    "        'I':{'E': -math.inf, '5': -math.inf, 'I': math.log(0.9), 'End':math.log(0.1)}\n",
    "    }\n",
    "    \n",
    "    # Emission probabilities (log space)\n",
    "    emit = {\n",
    "        'E': {'A': math.log(0.25), 'C': math.log(0.25), 'G': math.log(0.25), 'T':math.log(0.25)},\n",
    "        '5':{'A': math.log(0.05), 'C': -math.inf, 'G': math.log(0.95), 'T': -math.inf},\n",
    "        'I': {'A': math.log(0.4), 'C': math.log(0.1), 'G': math.log(0.1), 'T': math.log(0.4)}\n",
    "    }\n",
    "    \n",
    "    # Initialize Viterbi matrix and backpointer\n",
    "    V = [{}]\n",
    "    backpointer = [{}]\n",
    "    \n",
    "    # Initialization step (start in 'E')\n",
    "    for s in states:\n",
    "        V[0][s] = trans['Start'][s]+emit[s][sequence[0]] if s == 'E' else -math.inf\n",
    "        backpointer[0][s] = 'Start'\n",
    "    \n",
    "    # Recursion step\n",
    "    for t in range(1, len(sequence)):\n",
    "        V.append({})\n",
    "        backpointer.append({})\n",
    "        for s in states:\n",
    "            max_prob = -math.inf\n",
    "            best_prev =None\n",
    "            for prev_s in states:\n",
    "                prob =V[t-1][prev_s]+ trans[prev_s][s] +emit[s][sequence[t]]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_prev = prev_s\n",
    "            V[t][s] = max_prob\n",
    "            backpointer[t][s] = best_prev\n",
    "    \n",
    "    # Termination step (transition to 'End')\n",
    "    max_final = -math.inf\n",
    "    best_final_state = None\n",
    "    for s in states:\n",
    "        prob = V[-1][s] +trans[s]['End']\n",
    "        if prob >max_final:\n",
    "            max_final= prob\n",
    "            best_final_state= s\n",
    "    \n",
    "    # Backtracking\n",
    "    best_path = []\n",
    "    current_state = best_final_state\n",
    "    best_path.append(current_state)\n",
    "    for t in range(len(sequence)-1,0, -1):\n",
    "        best_path.insert(0,backpointer[t][current_state])\n",
    "        current_state= backpointer[t][current_state]\n",
    "    \n",
    "    return ''.join(best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28332ea3",
   "metadata": {},
   "source": [
    "Testing on sequence given in primer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c5bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEEEEEEEEEEEEEEEEE5IIIIIII\n"
     ]
    }
   ],
   "source": [
    "sequence = \"CTTCATGTGAAAGCAGACGTAAGTCA\" \n",
    "print(viterbi(sequence))  # Output depends on emissions (e.g., \"E5I\" or \"EEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a63c45",
   "metadata": {},
   "source": [
    "## Reason for using log space: \n",
    "1. Probabilities shrink exponentially with sequence length and computers cannot represent extremely small numbers.\n",
    "\n",
    "2. Log probabilities turn multiplication into addition (no underflow) which gives numerical         stability (adding log-probs is more stable than multiplying tiny floats).\n",
    "\n",
    "3. Comparing log-probs is equivalent to comparing raw probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
